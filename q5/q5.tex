\documentclass[12pt,a4paper]{article}
% Encoding + Language
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
% Math
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathtools}
% Graphics & Figures
\usepackage{graphicx}
\usepackage{float} % allows [H] for figures
% Page Layout
\usepackage{geometry}
\geometry{margin=1in}
% Colors
\usepackage{xcolor}

\title{Natural Language Processing - Assignment 2}
% ---------------------------------------------------------

\begin{document}

\maketitle

\newpage

\section{Question 5: Sentiment Analysis}

\subsection{Part (a) - T5 Paper Research}

\subsubsection{GitHub Repository}
The GitHub repository for the T5 project is:
\begin{center}
\texttt{https://github.com/google-research/text-to-text-transfer-transformer}
\end{center}
The repository was cloned locally. 

\subsubsection{Models Made Publicly Available}
The authors released five pre-trained T5 model variants with different sizes:

\begin{itemize}
    \item \textbf{T5-Small}: 60 million parameters
    \item \textbf{T5-Base}: 220 million parameters
    \item \textbf{T5-Large}: 770 million parameters
    \item \textbf{T5-3B}: 3 billion parameters
    \item \textbf{T5-11B}: 11 billion parameters
\end{itemize}

\noindent\textbf{Source:} GitHub README.md, section ``Released Model Checkpoints''.

\subsubsection{Dataset for Sentiment Analysis Benchmark}
The dataset used to benchmark sentiment analysis in the T5 paper is \textbf{SST-2 (Stanford Sentiment Treebank-2)}, which is part of the GLUE benchmark suite. SST-2 is a binary sentiment classification task.

\noindent\textbf{Source:} T5 Paper, Section 2.3 ``Downstream Tasks'', GLUE benchmark description. It shows all the different benchmarks used. 

\subsubsection{Evaluation Metric}
The evaluation metric used for the sentiment analysis task (SST-2) is \textbf{Accuracy}. The model's predictions are compared against the ground truth labels, and the percentage of correct predictions is reported.

\noindent\textbf{Source:} T5 Paper, Various tables showing SST-2 being evaluated based on accuracy, column header ``SST-2: Acc''.

\subsection{Part (b) - T5-Small Model Fine-tuned on SST2}

\subsubsection{Selected Model}
The selected model for this assignment is:
\begin{center}
\textbf{\texttt{lightsout19/t5-sst2}}
\end{center}

This model is a T5 small variant that has been fine-tuned on the SST-2 dataset for sentiment classification.

\noindent\textbf{Source:} Hugging Face Model Hub - \texttt{https://huggingface.co/lightsout19/t5-sst2}

\subsection{Part (c) - Sentiment Predictions on Four Sentences}
The results are in the Jupyter notebook, repeating for convince: 

\begin{center}
\begin{tabular}{|p{7cm}|c|c|}
\hline
\textbf{Sentence} & \textbf{Prediction} & \textbf{Confidence} \\
\hline
This movie is awesome & POSITIVE & 0.9992 \\
\hline
I didn't like the movie so much & NEGATIVE & 0.9897 \\
\hline
I'm not sure what I think about this movie. & NEGATIVE & 0.9766 \\
\hline
Did you like the movie? & POSITIVE & 0.9577 \\
\hline
\end{tabular}
\end{center}


\subsection{Part (d) - Model Evaluation on SST-2 Dataset}

We evaluated the model on the SST-2 validation dataset of 872 examples. The model achieved 90.14\% accuracy, correctly classifying 786 out of 872 examples. 
\subsection{Part (e) - Dataset Balance Analysis}

\subsubsection{Is the SST-2 Dataset Balanced?}
The SST-2 GLUE set is balanced, with 29780 negative examples (44.2\%) and 37569 positive examples (55.8\%) a balance ratio of 0.793. Which seems fairly well balanced. It appears the researchers have built the dataset to be balanced, as it is likely real distribution skews to a certain side more significantly.

\subsubsection{Why is Dataset Balance Important?}
Let us consider a hypothetical imbalanced dataset with 90\% positive and 10\% negative examples. A trivial classifier that always predicts ``positive'' would achieve 90\% accuracy despite not being 'intelligent' what so ever. 
\\
With SST-2's balanced distribution, our 90.14\% accuracy genuinely reflects the model's classification ability rather than dataset bias. Therefore, the dataset balance is very important in this case as without it bad classifiers can achieve good results, rendering the evaluation objectively poor.

\subsection{Part (f) - Limitations of Accuracy as an Evaluation Metric}

When evaluating sentiment predictions, human evaluators notice properties beyond correctness that accuracy doesn't measure:

\begin{enumerate}
    \item \textbf{Confidence Appropriateness:} Humans check if confidence matches prediction difficulty. Our model gives 95.77\% confidence to ``Did you like the movie?'' (a question with no sentiment), similar to clearly positive cases. Accuracy ignores whether high confidence is justified.
    
    \item \textbf{Error Patterns and Consistency:} Humans notice when similar inputs get inconsistent predictions (e.g., ``I liked it'' vs ``I really liked it''). Accuracy evaluates each example independently, missing systematic inconsistencies that suggest the model doesn't truly understand sentiment.
    
    \item \textbf{Understanding vs. Pattern Matching:} Humans want to know if the model understands negation, sarcasm, or context. ``I didn't like it'' being classified correctly could mean understanding negation or just pattern-matching ``didn't like.'' Accuracy only checks the final label.
\end{enumerate}


\subsection{Part (g) - Consulting for Healthcare Sentiment Analysis}

\textbf{Three Approaches:}
\\
\textbf{1. Use an Existing Sentiment Model:} Deploy the current SST-2 model directly or a similar model. Our 90.14\% accuracy shows the model works well on movie reviews, but medical summaries differ significantly in vocabulary and structure. Our results reveal concerning patterns: ``I'm not sure what I think about this movie'' was classified as negative with 97.66\% confidence, showing the model treats uncertainty as negativity which may be problematic for medical contexts where uncertainty is common.
\\
\textbf{2. Fine-tune on Labeled Medical Data:} 
Collect 500--1000 annotated patient visit summaries and fine-tune T5 or a medical domain model (For example BioBERT). The T5 paper (Section 3.7.2) showed domain-specific pre-training improved performance on in-domain tasks. However, annotation and development requires expertise and will likely be more costly and have a higher development time.
\\
\textbf{3. LLM Prompting:} Use GPT-4 or a similar LLM model with 5-10 example summaries in the prompt. No training required, but raises HIPAA compliance issues for patient data sent to external APIs. Consider local deployment for sensitive data. Running LLM models can also be costly and requires significant infrastructure if ran locally. Another downside is the unpredictably of LLM models, before deployment the LLM suitability and performance needs to be rigorously tested.
\\
\textbf{Recommendation:} Test approach 1 on 100 samples first to quantify domain mismatch. If performance is inadequate, invest in approach 2 with emphasis on per-supplier error analysis, not just overall accuracy. Alternatively Consider testing LLM models performance if they are tested adequate consistently and there is sufficient local infrastructure consider using them.

\end{document}

\end{document}
